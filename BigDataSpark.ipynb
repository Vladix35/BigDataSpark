{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1420507-2709-486d-ae91-94954ecaec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, row_number\n",
    "\n",
    "JDBC_HOST = \"doc_postgres\"\n",
    "JDBC_PORT = 5432\n",
    "JDBC_DB = \"postgres\"\n",
    "JDBC_USER = \"postgres\"\n",
    "JDBC_PASSWORD = \"mypassword\"\n",
    "JDBC_URL = f\"jdbc:postgresql://{JDBC_HOST}:{JDBC_PORT}/{JDBC_DB}\"\n",
    "JDBC_DRIVER = \"org.postgresql.Driver\"\n",
    "SRC_TABLE = \"mock_data\"\n",
    "\n",
    "def add_sequential_key(df, key_name, order_cols=None, start=1):\n",
    "    if order_cols is None:\n",
    "        df = df.withColumn(\"__tmp_mon\", monotonically_increasing_id())\n",
    "        w = Window.orderBy(col(\"__tmp_mon\"))\n",
    "        out = df.withColumn(key_name, row_number().over(w) + (start - 1)).drop(\"__tmp_mon\")\n",
    "    else:\n",
    "        w = Window.orderBy(*[col(c) for c in order_cols])\n",
    "        out = df.withColumn(key_name, row_number().over(w) + (start - 1))\n",
    "    return out\n",
    "\n",
    "def build_compact(session, df, jdbc_url, props):\n",
    "    dim_customer = (df.select(\n",
    "                        col(\"sale_customer_id\").alias(\"customer_business_id\"),\n",
    "                        col(\"customer_first_name\").alias(\"first_name\"),\n",
    "                        col(\"customer_last_name\").alias(\"last_name\"),\n",
    "                        col(\"customer_age\").alias(\"age\"),\n",
    "                        col(\"customer_email\").alias(\"email\"),\n",
    "                        col(\"customer_country\").alias(\"country\"),\n",
    "                        col(\"customer_postal_code\").alias(\"postal_code\"),\n",
    "                        col(\"customer_pet_type\").alias(\"pet_type\"),\n",
    "                        col(\"customer_pet_name\").alias(\"pet_name\"),\n",
    "                        col(\"customer_pet_breed\").alias(\"pet_breed\")\n",
    "                    )\n",
    "                    .filter(col(\"customer_business_id\").isNotNull())\n",
    "                    .dropDuplicates([\"customer_business_id\"]))\n",
    "    dim_customer = add_sequential_key(dim_customer, \"customer_key\", order_cols=[\"customer_business_id\"])\n",
    "    dim_customer.write.jdbc(url=jdbc_url, table=\"dim_customer\", mode=\"overwrite\", properties=props)\n",
    "\n",
    "    dim_product = (df.select(\n",
    "                        col(\"sale_product_id\").alias(\"product_business_id\"),\n",
    "                        col(\"product_name\"),\n",
    "                        col(\"product_category\").alias(\"category\"),\n",
    "                        col(\"product_price\").alias(\"price\"),\n",
    "                        col(\"product_weight\").alias(\"weight\"),\n",
    "                        col(\"product_color\").alias(\"color\"),\n",
    "                        col(\"product_size\").alias(\"size\"),\n",
    "                        col(\"product_brand\").alias(\"brand\"),\n",
    "                        col(\"product_material\").alias(\"material\"),\n",
    "                        col(\"product_description\").alias(\"description\"),\n",
    "                        col(\"product_rating\").alias(\"rating\"),\n",
    "                        col(\"product_reviews\").alias(\"reviews\"),\n",
    "                        col(\"product_release_date\").alias(\"release_date\"),\n",
    "                        col(\"product_expiry_date\").alias(\"expiry_date\"),\n",
    "                        col(\"pet_category\")\n",
    "                    )\n",
    "                    .filter(col(\"product_business_id\").isNotNull())\n",
    "                    .dropDuplicates([\"product_business_id\"]))\n",
    "    dim_product = add_sequential_key(dim_product, \"product_key\", order_cols=[\"product_business_id\"])\n",
    "    dim_product.write.jdbc(url=jdbc_url, table=\"dim_product\", mode=\"overwrite\", properties=props)\n",
    "\n",
    "    dim_seller = (df.select(\n",
    "                        col(\"sale_seller_id\").alias(\"seller_business_id\"),\n",
    "                        col(\"seller_first_name\").alias(\"first_name\"),\n",
    "                        col(\"seller_last_name\").alias(\"last_name\"),\n",
    "                        col(\"seller_email\").alias(\"email\"),\n",
    "                        col(\"seller_country\").alias(\"country\"),\n",
    "                        col(\"seller_postal_code\").alias(\"postal_code\")\n",
    "                    )\n",
    "                    .filter(col(\"seller_business_id\").isNotNull())\n",
    "                    .dropDuplicates([\"seller_business_id\"]))\n",
    "    dim_seller = add_sequential_key(dim_seller, \"seller_key\", order_cols=[\"seller_business_id\"])\n",
    "    dim_seller.write.jdbc(url=jdbc_url, table=\"dim_seller\", mode=\"overwrite\", properties=props)\n",
    "\n",
    "    dim_store = (df.select(\n",
    "                    col(\"store_name\"),\n",
    "                    col(\"store_location\").alias(\"location\"),\n",
    "                    col(\"store_city\").alias(\"city\"),\n",
    "                    col(\"store_state\").alias(\"state\"),\n",
    "                    col(\"store_country\").alias(\"country\"),\n",
    "                    col(\"store_phone\").alias(\"phone\"),\n",
    "                    col(\"store_email\").alias(\"email\")\n",
    "                  )\n",
    "                  .filter(col(\"store_name\").isNotNull() & col(\"store_location\").isNotNull())\n",
    "                  .dropDuplicates([\"store_name\", \"location\"]))\n",
    "    dim_store = add_sequential_key(dim_store, \"store_key\", order_cols=[\"store_name\", \"location\"])\n",
    "    dim_store.write.jdbc(url=jdbc_url, table=\"dim_store\", mode=\"overwrite\", properties=props)\n",
    "\n",
    "    dim_supplier = (df.select(\n",
    "                        col(\"supplier_name\"),\n",
    "                        col(\"supplier_contact\").alias(\"contact\"),\n",
    "                        col(\"supplier_email\").alias(\"email\"),\n",
    "                        col(\"supplier_phone\").alias(\"phone\"),\n",
    "                        col(\"supplier_address\").alias(\"address\"),\n",
    "                        col(\"supplier_city\").alias(\"city\"),\n",
    "                        col(\"supplier_country\").alias(\"country\")\n",
    "                    )\n",
    "                    .filter(col(\"supplier_name\").isNotNull())\n",
    "                    .dropDuplicates([\"supplier_name\"]))\n",
    "    dim_supplier = add_sequential_key(dim_supplier, \"supplier_key\", order_cols=[\"supplier_name\"])\n",
    "    dim_supplier.write.jdbc(url=jdbc_url, table=\"dim_supplier\", mode=\"overwrite\", properties=props)\n",
    "\n",
    "    dim_customer = session.read.jdbc(url=jdbc_url, table=\"dim_customer\", properties=props).alias(\"c\")\n",
    "    dim_product = session.read.jdbc(url=jdbc_url, table=\"dim_product\", properties=props).alias(\"p\")\n",
    "    dim_seller = session.read.jdbc(url=jdbc_url, table=\"dim_seller\", properties=props).alias(\"s\")\n",
    "    dim_store = session.read.jdbc(url=jdbc_url, table=\"dim_store\", properties=props).alias(\"st\")\n",
    "    dim_supplier = session.read.jdbc(url=jdbc_url, table=\"dim_supplier\", properties=props).alias(\"sup\")\n",
    "\n",
    "    m = df.alias(\"m\")\n",
    "    joined = m \\\n",
    "        .join(dim_customer, col(\"m.sale_customer_id\") == col(\"c.customer_business_id\"), \"left\") \\\n",
    "        .join(dim_seller, col(\"m.sale_seller_id\") == col(\"s.seller_business_id\"), \"left\") \\\n",
    "        .join(dim_product, col(\"m.sale_product_id\") == col(\"p.product_business_id\"), \"left\") \\\n",
    "        .join(dim_store, (col(\"m.store_name\") == col(\"st.store_name\")) & (col(\"m.store_location\") == col(\"st.location\")), \"left\") \\\n",
    "        .join(dim_supplier, col(\"m.supplier_name\") == col(\"sup.supplier_name\"), \"left\")\n",
    "\n",
    "    fact = joined.select(\n",
    "        col(\"c.customer_key\").alias(\"customer_key\"),\n",
    "        col(\"s.seller_key\").alias(\"seller_key\"),\n",
    "        col(\"p.product_key\").alias(\"product_key\"),\n",
    "        col(\"st.store_key\").alias(\"store_key\"),\n",
    "        col(\"sup.supplier_key\").alias(\"supplier_key\"),\n",
    "        col(\"m.sale_date\").alias(\"sale_date\"),\n",
    "        col(\"m.sale_quantity\").alias(\"quantity\"),\n",
    "        col(\"m.sale_total_price\").alias(\"total_price\"),\n",
    "        col(\"m.id\").alias(\"original_id\")\n",
    "    )\n",
    "\n",
    "    fact.write.jdbc(url=jdbc_url, table=\"fact_sale\", mode=\"overwrite\", properties=props)\n",
    "\n",
    "def run():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"compact_snowflake_spark\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    props = {\"user\": JDBC_USER, \"password\": JDBC_PASSWORD, \"driver\": JDBC_DRIVER}\n",
    "    src = spark.read.jdbc(url=JDBC_URL, table=SRC_TABLE, properties=props)\n",
    "    build_compact(spark, src, JDBC_URL, props)\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f14886d-facb-4595-b6b5-b5b903c34c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clickhouse_connect\n",
      "  Downloading clickhouse_connect-0.10.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from clickhouse_connect) (2023.7.22)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/conda/lib/python3.11/site-packages (from clickhouse_connect) (2.0.7)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from clickhouse_connect) (2023.3.post1)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.11/site-packages (from clickhouse_connect) (0.21.0)\n",
      "Requirement already satisfied: lz4 in /opt/conda/lib/python3.11/site-packages (from clickhouse_connect) (4.3.2)\n",
      "Downloading clickhouse_connect-0.10.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: clickhouse_connect\n",
      "Successfully installed clickhouse_connect-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install clickhouse_connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6666316-dc75-46ee-be86-d1861684e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, sum as _sum, avg as _avg, month, year, corr,\n",
    "    to_date, coalesce, when, lit, from_unixtime, length, lag\n",
    ")\n",
    "from pyspark.sql.types import StructType\n",
    "from clickhouse_connect import get_client\n",
    "from datetime import date, datetime\n",
    "from decimal import Decimal\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "POSTGRES_JDBC_URL = \"jdbc:postgresql://doc_postgres:5432/postgres\"\n",
    "PG_PROPS = {\"user\": \"postgres\", \"password\": \"mypassword\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "CLICKHOUSE_HOST = \"clickhouse-server\"\n",
    "CLICKHOUSE_PORT = 8123\n",
    "CH_USER = \"default\"\n",
    "CH_PASSWORD = \"\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Postgres->ClickHouse ETL (date-fix, nullable monthly)\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "ch = get_client(host=CLICKHOUSE_HOST, port=CLICKHOUSE_PORT, username=CH_USER, password=CH_PASSWORD)\n",
    "\n",
    "def read_from_postgres(table):\n",
    "    df = spark.read.jdbc(url=POSTGRES_JDBC_URL, table=table, properties=PG_PROPS)\n",
    "    return df.cache()\n",
    "\n",
    "def recreate_ch_table(name, ddl_body):\n",
    "    ch.command(f\"DROP TABLE IF EXISTS {name}\")\n",
    "    sql = f\"CREATE TABLE {name} ({ddl_body}) ENGINE = MergeTree() ORDER BY tuple()\"\n",
    "    ch.command(sql)\n",
    "\n",
    "def _normalize_value(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (date, datetime)):\n",
    "        return v.isoformat()\n",
    "    if isinstance(v, Decimal):\n",
    "        return float(v)\n",
    "    try:\n",
    "        if hasattr(v, \"item\"):\n",
    "            return v.item()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return v\n",
    "\n",
    "def insert_spark_df_to_ch(df, table_name, batch_size=1000):\n",
    "    cols = df.columns\n",
    "    if not cols:\n",
    "        return\n",
    "    it = df.toLocalIterator()\n",
    "    batch = []\n",
    "    total = 0\n",
    "    for row in it:\n",
    "        tup = tuple(_normalize_value(row[i]) for i in range(len(cols)))\n",
    "        batch.append(tup)\n",
    "        if len(batch) >= batch_size:\n",
    "            ch.insert(table_name, batch, column_names=cols)\n",
    "            total += len(batch)\n",
    "            batch = []\n",
    "    if batch:\n",
    "        ch.insert(table_name, batch, column_names=cols)\n",
    "        total += len(batch)\n",
    "\n",
    "fact_sale = read_from_postgres(\"fact_sale\")\n",
    "dim_product = read_from_postgres(\"dim_product\")\n",
    "dim_customer = read_from_postgres(\"dim_customer\")\n",
    "dim_store = read_from_postgres(\"dim_store\")\n",
    "dim_supplier = read_from_postgres(\"dim_supplier\")\n",
    "\n",
    "possible_date_cols = [\"sale_date\", \"sale_ts\", \"sale_datetime\", \"created_at\", \"date\", \"timestamp\"]\n",
    "formats = [\n",
    "    \"M/d/yyyy\", \"MM/dd/yyyy\", \"yyyy-MM-dd HH:mm:ss\",\n",
    "    \"yyyy-MM-dd\", \"dd.MM.yyyy\", \"yyyy/MM/dd\", \"MMM d, yyyy\"\n",
    "]\n",
    "\n",
    "def build_parsed_date_expr(col_name):\n",
    "    parsed_expr = None\n",
    "    s = col(col_name).cast(\"string\")\n",
    "    for fmt in formats:\n",
    "        parsed = to_date(s, fmt)\n",
    "        parsed_expr = parsed if parsed_expr is None else coalesce(parsed_expr, parsed)\n",
    "    parsed_epoch_secs = when((s.rlike('^[0-9]+$')) & (length(s) <= 10),\n",
    "                             to_date(from_unixtime(s.cast(\"long\"))))\n",
    "    parsed_expr = coalesce(parsed_expr, parsed_epoch_secs)\n",
    "    parsed_epoch_millis = when((s.rlike('^[0-9]+$')) & (length(s) > 10),\n",
    "                               to_date(from_unixtime((s.cast(\"double\")/1000).cast(\"long\"))))\n",
    "    parsed_expr = coalesce(parsed_expr, parsed_epoch_millis)\n",
    "    parsed_expr = coalesce(parsed_expr, to_date(s))\n",
    "    return parsed_expr\n",
    "\n",
    "parsed_expr = None\n",
    "for c in possible_date_cols:\n",
    "    if c in fact_sale.columns:\n",
    "        expr = build_parsed_date_expr(c)\n",
    "        parsed_expr = expr if parsed_expr is None else coalesce(parsed_expr, expr)\n",
    "\n",
    "if parsed_expr is not None:\n",
    "    fact_sale = fact_sale.withColumn(\"sale_date_parsed\", parsed_expr)\n",
    "    fact_sale = fact_sale.withColumn(\"sale_date\", col(\"sale_date_parsed\")).drop(\"sale_date_parsed\")\n",
    "\n",
    "if fact_sale.columns and dim_product.columns:\n",
    "    top_products = fact_sale.join(dim_product, \"product_key\", \"inner\") \\\n",
    "        .groupBy(\"product_name\") \\\n",
    "        .agg(_sum(col(\"quantity\")).alias(\"total_quantity_sold\")) \\\n",
    "        .orderBy(col(\"total_quantity_sold\").desc()).limit(10)\n",
    "    recreate_ch_table(\"top_products_mart\", \"product_name String, total_quantity_sold Float64\")\n",
    "    insert_spark_df_to_ch(top_products.select(\"product_name\", \"total_quantity_sold\"), \"top_products_mart\")\n",
    "\n",
    "    category_revenue = fact_sale.join(dim_product, \"product_key\", \"inner\") \\\n",
    "        .groupBy(\"category\") \\\n",
    "        .agg(_sum(col(\"total_price\")).alias(\"total_revenue\"))\n",
    "    recreate_ch_table(\"category_revenue_mart\", \"category String, total_revenue Float64\")\n",
    "    insert_spark_df_to_ch(category_revenue.select(\"category\", \"total_revenue\"), \"category_revenue_mart\")\n",
    "\n",
    "    product_ratings = fact_sale.join(dim_product, \"product_key\", \"inner\") \\\n",
    "        .groupBy(\"product_name\") \\\n",
    "        .agg(_avg(col(\"rating\")).alias(\"avg_rating\"), _sum(col(\"reviews\")).alias(\"total_reviews\"))\n",
    "    recreate_ch_table(\"product_ratings_mart\", \"product_name String, avg_rating Float64, total_reviews Int64\")\n",
    "    insert_spark_df_to_ch(product_ratings.select(\"product_name\", \"avg_rating\", \"total_reviews\"), \"product_ratings_mart\")\n",
    "\n",
    "if fact_sale.columns and dim_customer.columns:\n",
    "    top_customers = fact_sale.join(dim_customer, \"customer_key\", \"inner\") \\\n",
    "        .groupBy(\"first_name\", \"last_name\") \\\n",
    "        .agg(_sum(col(\"total_price\")).alias(\"total_spent\")) \\\n",
    "        .orderBy(col(\"total_spent\").desc()).limit(10)\n",
    "    recreate_ch_table(\"top_customers_mart\", \"first_name String, last_name String, total_spent Float64\")\n",
    "    insert_spark_df_to_ch(top_customers.select(\"first_name\", \"last_name\", \"total_spent\"), \"top_customers_mart\")\n",
    "\n",
    "    customers_by_country = fact_sale.join(dim_customer, \"customer_key\", \"inner\") \\\n",
    "        .groupBy(\"country\") \\\n",
    "        .agg(_sum(col(\"total_price\")).alias(\"country_revenue\"))\n",
    "    recreate_ch_table(\"customers_by_country_mart\", \"country String, country_revenue Float64\")\n",
    "    insert_spark_df_to_ch(customers_by_country.select(\"country\", \"country_revenue\"), \"customers_by_country_mart\")\n",
    "\n",
    "    customer_avg_check = fact_sale.join(dim_customer, \"customer_key\", \"inner\") \\\n",
    "        .groupBy(\"first_name\", \"last_name\") \\\n",
    "        .agg(_avg(col(\"total_price\")).alias(\"avg_check\"))\n",
    "    recreate_ch_table(\"customer_avg_check_mart\", \"first_name String, last_name String, avg_check Float64\")\n",
    "    insert_spark_df_to_ch(customer_avg_check.select(\"first_name\", \"last_name\", \"avg_check\"), \"customer_avg_check_mart\")\n",
    "\n",
    "if \"sale_date\" in fact_sale.columns and fact_sale.filter(col(\"sale_date\").isNotNull()).count() > 0:\n",
    "    fact_with_date = fact_sale.filter(col(\"sale_date\").isNotNull())\n",
    "\n",
    "    monthly = fact_with_date \\\n",
    "        .withColumn(\"sale_year\", year(col(\"sale_date\")).cast(\"int\")) \\\n",
    "        .withColumn(\"sale_month\", month(col(\"sale_date\")).cast(\"int\")) \\\n",
    "        .groupBy(\"sale_year\", \"sale_month\") \\\n",
    "        .agg(_sum(col(\"total_price\")).alias(\"monthly_revenue\")) \\\n",
    "        .orderBy(\"sale_year\", \"sale_month\")\n",
    "\n",
    "    w = Window.orderBy(\"sale_year\", \"sale_month\")\n",
    "    monthly = monthly.withColumn(\"prev_month_revenue\", lag(\"monthly_revenue\").over(w))\n",
    "    monthly = monthly.withColumn(\"mom_change_pct\",\n",
    "                                 when(col(\"prev_month_revenue\").isNotNull() & (col(\"prev_month_revenue\") != 0),\n",
    "                                      (col(\"monthly_revenue\") - col(\"prev_month_revenue\")) / col(\"prev_month_revenue\"))\n",
    "                                 .otherwise(lit(None))\n",
    "                                 )\n",
    "\n",
    "    recreate_ch_table(\n",
    "        \"monthly_trends_mart\",\n",
    "        \"sale_year Int32, sale_month Int32, monthly_revenue Float64, prev_month_revenue Nullable(Float64), mom_change_pct Nullable(Float64)\"\n",
    "    )\n",
    "    insert_spark_df_to_ch(monthly.select(\"sale_year\", \"sale_month\", \"monthly_revenue\", \"prev_month_revenue\", \"mom_change_pct\"),\n",
    "                          \"monthly_trends_mart\")\n",
    "\n",
    "    yearly = monthly.groupBy(\"sale_year\").agg(_sum(\"monthly_revenue\").alias(\"yearly_revenue\")).orderBy(\"sale_year\")\n",
    "    recreate_ch_table(\"yearly_trends_mart\", \"sale_year Int32, yearly_revenue Float64\")\n",
    "    insert_spark_df_to_ch(yearly.select(\"sale_year\", \"yearly_revenue\"), \"yearly_trends_mart\")\n",
    "\n",
    "    avg_order_monthly = fact_with_date \\\n",
    "        .withColumn(\"sale_year\", year(col(\"sale_date\")).cast(\"int\")) \\\n",
    "        .withColumn(\"sale_month\", month(col(\"sale_date\")).cast(\"int\")) \\\n",
    "        .groupBy(\"sale_year\", \"sale_month\") \\\n",
    "        .agg(_avg(col(\"total_price\")).alias(\"avg_order_value\"))\n",
    "    recreate_ch_table(\"avg_order_monthly_mart\", \"sale_year Int32, sale_month Int32, avg_order_value Nullable(Float64)\")\n",
    "    insert_spark_df_to_ch(avg_order_monthly.select(\"sale_year\", \"sale_month\", \"avg_order_value\"), \"avg_order_monthly_mart\")\n",
    "\n",
    "if fact_sale.columns and dim_store.columns:\n",
    "    top_stores = fact_sale.join(dim_store, \"store_key\", \"inner\") \\\n",
    "        .groupBy(\"store_name\") \\\n",
    "        .agg(_sum(col(\"total_price\")).alias(\"total_revenue\")) \\\n",
    "        .orderBy(col(\"total_revenue\").desc()).limit(5)\n",
    "    recreate_ch_table(\"top_stores_mart\", \"store_name String, total_revenue Float64\")\n",
    "    insert_spark_df_to_ch(top_stores.select(\"store_name\", \"total_revenue\"), \"top_stores_mart\")\n",
    "\n",
    "    sales_by_city = fact_sale.join(dim_store, \"store_key\", \"inner\") \\\n",
    "        .groupBy(\"city\", \"country\") \\\n",
    "        .agg(_sum(col(\"total_price\")).alias(\"city_revenue\"))\n",
    "    recreate_ch_table(\"sales_by_city_mart\", \"city String, country String, city_revenue Float64\")\n",
    "    insert_spark_df_to_ch(sales_by_city.select(\"city\", \"country\", \"city_revenue\"), \"sales_by_city_mart\")\n",
    "\n",
    "    store_avg_check = fact_sale.join(dim_store, \"store_key\", \"inner\") \\\n",
    "        .groupBy(\"store_name\") \\\n",
    "        .agg(_avg(col(\"total_price\")).alias(\"avg_receipt\"))\n",
    "    recreate_ch_table(\"store_avg_check_mart\", \"store_name String, avg_receipt Float64\")\n",
    "    insert_spark_df_to_ch(store_avg_check.select(\"store_name\", \"avg_receipt\"), \"store_avg_check_mart\")\n",
    "\n",
    "if fact_sale.columns and dim_supplier.columns:\n",
    "    top_suppliers = fact_sale.join(dim_supplier, \"supplier_key\", \"inner\") \\\n",
    "        .groupBy(\"supplier_name\") \\\n",
    "        .agg(_sum(col(\"total_price\")).alias(\"total_revenue\")) \\\n",
    "        .orderBy(col(\"total_revenue\").desc()).limit(5)\n",
    "    recreate_ch_table(\"top_suppliers_mart\", \"supplier_name String, total_revenue Float64\")\n",
    "    insert_spark_df_to_ch(top_suppliers.select(\"supplier_name\", \"total_revenue\"), \"top_suppliers_mart\")\n",
    "\n",
    "    supplier_avg_price = fact_sale.join(dim_supplier, \"supplier_key\", \"inner\") \\\n",
    "        .join(dim_product, \"product_key\", \"inner\") \\\n",
    "        .groupBy(\"supplier_name\") \\\n",
    "        .agg(_avg(col(\"price\")).alias(\"avg_price\"))\n",
    "    recreate_ch_table(\"supplier_avg_price_mart\", \"supplier_name String, avg_price Float64\")\n",
    "    insert_spark_df_to_ch(supplier_avg_price.select(\"supplier_name\", \"avg_price\"), \"supplier_avg_price_mart\")\n",
    "\n",
    "    supplier_by_country = fact_sale.join(dim_supplier, \"supplier_key\", \"inner\") \\\n",
    "        .groupBy(\"country\") \\\n",
    "        .agg(_sum(col(\"total_price\")).alias(\"country_revenue\"))\n",
    "    recreate_ch_table(\"supplier_by_country_mart\", \"country String, country_revenue Float64\")\n",
    "    insert_spark_df_to_ch(supplier_by_country.select(\"country\", \"country_revenue\"), \"supplier_by_country_mart\")\n",
    "\n",
    "if fact_sale.columns and dim_product.columns:\n",
    "    best_products = fact_sale.join(dim_product, \"product_key\", \"inner\") \\\n",
    "        .groupBy(\"product_name\") \\\n",
    "        .agg(_avg(col(\"rating\")).alias(\"avg_rating\")) \\\n",
    "        .orderBy(col(\"avg_rating\").desc()).limit(5)\n",
    "    recreate_ch_table(\"best_products_mart\", \"product_name String, avg_rating Float64\")\n",
    "    insert_spark_df_to_ch(best_products.select(\"product_name\", \"avg_rating\"), \"best_products_mart\")\n",
    "\n",
    "    worst_products = fact_sale.join(dim_product, \"product_key\", \"inner\") \\\n",
    "        .groupBy(\"product_name\") \\\n",
    "        .agg(_avg(col(\"rating\")).alias(\"avg_rating\")) \\\n",
    "        .orderBy(col(\"avg_rating\").asc()).limit(5)\n",
    "    recreate_ch_table(\"worst_products_mart\", \"product_name String, avg_rating Float64\")\n",
    "    insert_spark_df_to_ch(worst_products.select(\"product_name\", \"avg_rating\"), \"worst_products_mart\")\n",
    "\n",
    "    top_reviewed = fact_sale.join(dim_product, \"product_key\", \"inner\") \\\n",
    "        .groupBy(\"product_name\") \\\n",
    "        .agg(_sum(col(\"reviews\")).alias(\"total_reviews\")) \\\n",
    "        .orderBy(col(\"total_reviews\").desc()).limit(5)\n",
    "    recreate_ch_table(\"top_reviewed_mart\", \"product_name String, total_reviews Int64\")\n",
    "    insert_spark_df_to_ch(top_reviewed.select(\"product_name\", \"total_reviews\"), \"top_reviewed_mart\")\n",
    "\n",
    "    product_sales_correlation = fact_sale.join(dim_product, \"product_key\", \"inner\") \\\n",
    "        .groupBy(\"product_name\") \\\n",
    "        .agg(\n",
    "            _avg(col(\"rating\")).alias(\"avg_rating\"),\n",
    "            _sum(col(\"quantity\")).alias(\"total_quantity_sold\"),\n",
    "            _sum(col(\"total_price\")).alias(\"total_revenue\")\n",
    "        )\n",
    "    corr_row = product_sales_correlation.select(\n",
    "        corr(\"avg_rating\", \"total_quantity_sold\").alias(\"corr_q\"),\n",
    "        corr(\"avg_rating\", \"total_revenue\").alias(\"corr_r\")\n",
    "    ).collect()\n",
    "    corr_q = corr_row[0][\"corr_q\"] if corr_row and corr_row[0][\"corr_q\"] is not None else 0.0\n",
    "    corr_r = corr_row[0][\"corr_r\"] if corr_row and corr_row[0][\"corr_r\"] is not None else 0.0\n",
    "\n",
    "    recreate_ch_table(\"product_rating_correlation_mart\", \"metric String, correlation_value Float64\")\n",
    "    corr_df = spark.createDataFrame([(\"rating_vs_quantity\", float(corr_q)), (\"rating_vs_revenue\", float(corr_r))])\n",
    "    insert_spark_df_to_ch(corr_df.selectExpr(\"_1 as metric\", \"_2 as correlation_value\"), \"product_rating_correlation_mart\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e496d70-4446-4497-8f38-6ee7cc6e3d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
